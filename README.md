# NLP-Task4

Датасет брался из двух источников   
1 Haggle датасет https://www.kaggle.com/datasets/bfbarry/haiku-dataset  
2 При помощи парсинга взяли с сайта tenthousandhaiku   
3 И еще один датасет с haggle
В итоге размер датасета составляет более 13 тысяч хокку

Так как  в начальном виде они представляют собой просто перечесление хокку, необходимо было подобрать для них ключевые фразы, из-за размера Датасета мы посчитали невозможным вручную определить ключевые фразы для всех хокку, поэтому было принято решение использовать модели машинного обучения, для определение ключевых фраз из хокку и тона всего стиха, для этого использовались sentiment_pipeline от hugging face для определения тона и Rake-Nltk для нахождения ключевых фраз.   
Было решено  использовать 2 ключевые фразы и тон для генерации хокку    

Сначала запускается код из file parsing для подготовки второго датасета, затем код из Дополнительный датасет
